# 2025 05 29

- [ ] Learn more about react. 
- [ ] watch andrej GPT 2 serires. 
- [ ] German shengen visa application 
- [ ] Travel grant application
  - [ ] Follow up with wolfywolf for travel details
- [ ] Order keyboard components (see 05_27_2025.md)

------------------------

Ok, app developemnt is going to take a back seat. Let's focus on learning more about AI. 

By the way, Andrej has the outline for a course planed out from Eureka labs, but only the outline. It's kind of surprising to see how slow he's moving. I wonder why though. 
His content tends to be of great quality, but the time required in the background is perhaps sufficient. 

------------------

Ok, one thing that comes to mind is that I can just let Gemini fill in the blanks. Why don't we do that.

So it seems like Bigram models are models that predict the next word based on *only* the previous word. 

Wait, so if I have a sequence 
starting with a word (say, "the"),

then the sequence is probabilistically selected based off of the probabililties, so 
for example, the sequence 
"the cat ate the fish" 
is expected to appear 
$P(cat | the) * P(ate | cat) * \dots * P(fish | the)$ times. 

Why don't we start with this? Because this seems a bit boring, ngl. 

---------------------------

Let's see what the next step is:












