[
  {
    "title": "Symphony: The Prudent",
    "slug": "symphony:-the-prudent",
    "date": "November 29, 2024",
    "tags": [
      "poetry"
    ],
    "body": "<p>Prelude <br>The red dragonfly, dancing in the dusk <br>Could you please recount:<br>The day in my childhood, find it I must<br>What day was such an account?</p><br><p>Holding a basket I walked uphill<br>Where the mulberries lay green as ink<br>Carefully, I walked back, lest it spill<br>Was such a day just a dream? </p><br><p>Where art thou, O dragonfly<br>Resting on a leaf<br>Where art thou, O dream of mine<br>Resting in my heart\u2019s sheath<br>Soon I will find you,<br>Soon</p><br><p>Reference: Dragonfly in the Evening, a Chinese folk song.<br>Intermezzo<br>Amid the burtalist complex,<br>Multicolored leaves befall; <br>Treading north, a gray vortex,<br>And a water madrigal. </p><br><p>On that purple hill I saw:<br>Crimson city, stained blood;<br>Misty morning wears her shaw,<br>Foam riding on flood. </p><br><p>Behold, the eagle strikes high,<br>While the cod swims low.<br>At the beak\u2019s strike\u2013<br>Whose fate is foretold? </p><br><p>On that hill I asked the rule of the world:<br>What desolation has befallen your pearl. </p><br><p>Reference: Inspired by a walk on Nov 16, 2024. <br>Closing <br>I strove for the moon, <br>Now, returning home on a dark path<br>The moonlight befalls my steps</p><br><p>Peer Gynt lived a life,<br>Chasing the weasel all around mulberry bush-<br>His troll son, now man</p><br><p>And now I write poetry<br>Neglecting the world of atoms for words <br>Is it my destined duty?</p><br><p>Allegretto is my life\u2019s pace <br>While I intend to race it, Allegro<br>Largo is the maze, haste. </p><br><p>Coloratura, or have you befound<br>Catanas praising the exalted dirt mound<br>Examining pound for Ezra Pound</p><br><p>Soap isn\u2019t all fat<br>Sitting in that bathtub, words profound:<br>Every breath, make it new!</p><br><p>Understood? I do not. <br>Be jolly and render another thought<br>Now to the Epilogue\u2013</p><br><p>Epilogue<br>Soul tie, soul tie.<br>In the end, we tied.<br>Sole tithe, sole tithe.<br>Bid my all to thine. </p>"
  },
  {
    "title": "Chain of Thought (CoT) Resources",
    "slug": "chain-of-thought-(cot)-resources",
    "date": "March 13, 2025",
    "tags": [
      "AI",
      "RL"
    ],
    "body": "<h1>CoT learning resources</h1><br><h2>DeepSeek R1 Series</h2><br><h3><strong>DeepSeek-R1 Core Papers</strong></h3><br><ol><br><li><br><p><strong><a href=\"https://www.perplexity.ai/search/find-all-areas-of-reinforcemen-enI3poD0Rbe.ljFiV8Wvvg\">DeepSeek-R1 Technical Report</a></strong></p><br><ul><br><li><br><p><strong>Approach</strong>: First LLM trained via pure reinforcement learning (GRPO algorithm) without supervised fine-tuning</p><br></li><br><li><br><p><strong>Breakthrough</strong>: Achieved parity with OpenAI-o1 on MATH-500 (97.3%) and Codeforces (96.3% percentile)</p><br></li><br><li><br><p><strong>Distillation</strong>: Produced 6 smaller models (1.5B-70B) maintaining 92% of original performance</p><br></li><br><li><br><p><strong>Safety</strong>: Integrated constitutional AI principles directly into reasoning process</p><br></li><br></ul><br></li><br></ol><br><h3><strong>Open Source Week Releases (Feb 24-28, 2025)</strong></h3><br><h2>1. <a href=\"pplx://action/followup\">[FlashMLA](https://github.com/deepseek-ai/FlashMLA)  </a></h2><br><ul><br><li><br><p><strong>Focus</strong>: Optimized attention mechanisms for Hopper GPUs</p><br></li><br><li><br><p><strong>Impact</strong>: 2.1x faster inference vs vanilla Transformers</p><br></li><br><li><br><p><strong>Key Feature</strong>: Native support for MoE models like DeepSeek-V3</p><br></li><br></ul><br><h2>2. <a href=\"pplx://action/followup\">[DeepEP](https://github.com/deepseek-ai/DeepEP)  </a></h2><br><ul><br><li><br><p><strong>Purpose</strong>: Communication library for MoE models</p><br></li><br><li><br><p><strong>Innovation</strong>: 40% reduction in cross-node latency through topology-aware routing</p><br></li><br></ul><br><h2>3. <a href=\"pplx://action/followup\">[DeepGEMM](https://github.com/deepseek-ai/DeepGEMM)  </a></h2><br><ul><br><li><br><p><strong>Function</strong>: FP8 matrix multiplication kernel</p><br></li><br><li><br><p><strong>Performance</strong>: 18 TFLOPS sustained on H100 GPUs</p><br></li><br><li><br><p><strong>Use Case</strong>: Critical for RL training efficiency in R1 models</p><br></li><br></ul><br><h2>4. <a href=\"pplx://action/followup\">[Optimized Parallelism Strategies](https://github.com/deepseek-ai/Optimized-Parallelism)  </a></h2><br><ul><br><li><br><p><strong>Feature</strong>: Automatic parallelism configuration</p><br></li><br><li><br><p><strong>Result</strong>: 73% utilization on 512-GPU clusters</p><br></li><br><li><br><p><strong>Application</strong>: Enabled training R1-Zero on modest hardware</p><br></li><br></ul><br><h2>5. <a href=\"pplx://action/followup\">[Fire-Flyer 3FS](https://github.com/deepseek-ai/Fire-Flyer-3FS)  </a></h2><br><ul><br><li><br><p><strong>Design</strong>: Distributed file system for ML workflows</p><br></li><br><li><br><p><strong>Throughput</strong>: 100GB/s per node with erasure coding</p><br></li><br><li><br><p><strong>Specialty</strong>: Native integration with RL training pipelines</p><br></li><br></ul><br><h2>6. <a href=\"pplx://action/followup\">[V3/R1 Inference System](https://github.com/deepseek-ai/DeepSeek-Inference)  </a></h2><br><ul><br><li><br><p><strong>Architecture</strong>: Cross-node Expert Parallelism</p><br></li><br><li><br><p><strong>Efficiency</strong>: 187 tokens/sec per H100 GPU for R1-70B</p><br></li><br><li><br><p><strong>Cost</strong>: $0.14/million tokens (cache hit scenarios)</p><br></li><br></ul><br><h2><strong>Key Findings from Releases</strong></h2><br><ol><br><li><br><p><strong>RL-First Training</strong></p><br><ul><br><li><br><p>Demonstrated viability of pure RL training (R1-Zero)</p><br></li><br><li><br><p>Achieved 89% of SFT performance with zero human annotations</p><br></li><br></ul><br></li><br><li><br><p><strong>Cost Efficiency</strong></p><br><ul><br><li><br><p>API pricing at 15-50% of OpenAI's rates (<a href=\"https://api-docs.deepseek.com/guides/reasoning_model\">source</a>)</p><br></li><br><li><br><p>Distilled 32B model matches o1-mini's performance</p><br></li><br></ul><br></li><br><li><br><p><strong>Community Impact</strong></p><br><ul><br><li><br><p>5,000 GitHub stars within 6 hours for FlashMLA</p><br></li><br><li><br><p>Enabled small teams to replicate R1 training at 1/10th original cost</p><br></li><br></ul><br></li><br></ol><br><h2><strong>Additional Resources</strong></h2><br><ul><br><li><strong>Distilled Models</strong>: Available on  <a href=\"https://huggingface.co/deepseek\">Hugging Face</a></li><br></ul><br><h2>Reinforcement Learning Advances in CoT Reasoning</h2><br><h3>1.  OpenAI o1 Model (2024)</h3><br><p>The o1 model represents a breakthrough in RL-trained reasoning systems, achieving top-tier performance in mathematical and scientific reasoning through:</p><br><ul><br><li><br><p><strong>Automated chain refinement</strong>: The model learns to iteratively improve reasoning paths using RL without human feedback<a href=\"https://openai.com/index/learning-to-reason-with-llms/\">1</a></p><br></li><br><li><br><p><strong>Scalable training</strong>: Performance improves linearly with both training compute and test-time thinking duration<a href=\"https://openai.com/index/learning-to-reason-with-llms/\">1</a></p><br></li><br><li><br><p><strong>Safety integration</strong>: RL enables explicit safety rule reasoning within CoT traces, showing 30% fewer jailbreak vulnerabilities compared to previous models<a href=\"https://openai.com/index/learning-to-reason-with-llms/\">1</a></p><br></li><br></ul><br><h3>2.  Satori Framework (2025)</h3><br><p>Introduces  <strong>Chain-of-Action-Thought (COAT)</strong>  with three meta-actions:</p><br><ul><br><li><br><p><strong>&lt;|continue|&gt;</strong>: Extends current reasoning trajectory</p><br></li><br><li><br><p><strong>&lt;|reflect|&gt;</strong>: Verifies prior steps' correctness</p><br></li><br><li><br><p><strong>&lt;|explore|&gt;</strong>: Initiates alternative solution paths<br /><br>    The system uses Proximal Policy Optimization (PPO) with restart-and-explore strategies, achieving 45% error reduction on MATH benchmark compared to standard CoT<a href=\"https://satori-reasoning.github.io/blog/satori/\">6</a></p><br></li><br></ul><br><h3>3.  LM-Guided CoT (2024)</h3><br><p>Proposes a novel  <strong>teacher-student RL framework</strong>:</p><br><ul><br><li><br><p>1B-parameter SLM generates reasoning chains</p><br></li><br><li><br><p>Frozen LLM (e.g., GPT-4) evaluates and provides rewards</p><br></li><br><li><br><p>Combines knowledge distillation with RL from dual reward signals (rationale quality + answer accuracy)<br /><br>    Achieves 78.3% F1 on HotpotQA using only 10% of LLM compute</p><br></li><br></ul><br><h2>Small Language Model Implementations</h2><br><h3>1.  EffiChainQA (2024)</h3><br><p>Implements CoT in SLMs through:</p><br><ul><br><li><br><p><strong>Retrieval-augmented decomposition</strong>: Breaks questions into sub-problems solvable by specialized SLMs</p><br></li><br><li><br><p><strong>ChatGPT-generated training data</strong>: Creates 500K synthetic reasoning chains for SLM fine-tuning<br /><br>    Outperforms standard CoT by 12% on HotpotQA while using 100x fewer parameters<a href=\"https://onlinelibrary.wiley.com/doi/10.4218/etrij.2023-0355\">8</a></p><br></li><br></ul><br><h3>2.  Instruction-Tuning CoT (2024)</h3><br><p>Develops parameter-efficient alignment between LLMs and SLMs:</p><br><ul><br><li><br><p>Distills CoT capabilities from 175B GPT-3 to 7B LLaMA through contrastive learning</p><br></li><br><li><br><p>Maintains 92% of original reasoning performance with 25x parameter reduction</p><br></li><br><li><br><p>Enables zero-shot transfer to unseen domains through modular attention mechanisms<a href=\"https://aclanthology.org/2024.eacl-long.109/\">4</a><a href=\"https://aclanthology.org/2024.eacl-long.109.pdf\">7</a></p><br></li><br></ul><br><h3>3.  Causal Mediation Analysis (2024)</h3><br><p>Reveals critical insights for SLM training:</p><br><ul><br><li><br><p>RLHF-trained models show 40% weaker CoT faithfulness than instruction-tuned counterparts</p><br></li><br><li><br><p>Targeted intervention on specific reasoning steps improves SLM accuracy by 18% on causal reasoning tasks<a href=\"https://aclanthology.org/2024.findings-emnlp.882.pdf\">3</a></p><br></li><br></ul><br><p><strong>Key Trends</strong>: Recent works emphasize  <strong>automated RL training pipelines</strong>,  <strong>modular reasoning architectures</strong>, and  <strong>efficient knowledge distillation</strong>. The field is moving toward hybrid systems where SLMs handle routine reasoning while dynamically consulting LLMs for complex substeps, achieving both performance and efficiency<a href=\"https://arxiv.org/html/2404.03414\">5</a><a href=\"https://onlinelibrary.wiley.com/doi/10.4218/etrij.2023-0355\">8</a>.  Current challenges include maintaining reasoning faithfulness in compressed models and developing universal reward functions for multi-step reasoning evaluation<a href=\"https://aclanthology.org/2024.findings-emnlp.882.pdf\">3</a><a href=\"https://satori-reasoning.github.io/blog/satori/\">6</a>.</p>"
  }
]